\documentclass{amsart}
%\usepackage[left=3cm,right=3cm,nohead,bottom=3cm]{geometry}
\usepackage{mathpazo}
\usepackage{microtype}

\title{Type Inference using Deep Learning\\15-780 Project Progress Report}
\author{Ankush Das}
\author{Costin B\u{a}descu}
\author{Jonathan Sterling}
\author{Ryan Kavanagh}

\begin{document}
\maketitle

\section{Background and Problem}
Type inference refers to the deduction of the type of an expression in a
typed programming language. Type inference is usually implemented
using the well-known Hindley--Milner algorithm.
Unfortunately, in more complex type systems (e.g. System
F), type inference is often an undecidable
problem.

Our project explores the feasibility of using deep learning to infer
types of well-formed expressions in typed programming
languages. Because our project is focussed on the feasibility aspect,
we decided to begin with the simplest possible framework, namely, the
simply-typed $\lambda$-calculus (STLC) with the unit type as its only
base type. Formally, the types and expressions in our language is
defined with the following grammar.
\begin{eqnarray*}
\tau & ::= & \mathbf{1} \mid \tau \to \tau \\
e & ::= & * \mid \lambda x. e \mid e \; e
\end{eqnarray*}
The base expression $*$ has type $\mathbf{1}$, $\lambda x. e$ has type
$\tau_1 \to \tau_2$, if $e$ has the type $\tau_2$ given that $x$ has type
$\tau_1$. Similarly, $e_1 \; e_2$ has the type $\tau_2$ is $e_1$ has the
type $\tau_1 \to \tau_2$ and $e_2$ has the type $\tau_1$.

\section{Accomplishments}

We began by first formalising the STLC in Standard ML. This involved
making a few design choices that we believe would increase our chances
of success. For example, variable binding is often implemented using
explicit variable names. For example, the abstraction $\lambda x. xx$
binds the variable $x$ in the term $xx$. However, one can freely vary
bound variables (intuitively, $f(x) = x^2$ and $f(y) = y^2$ denote the
same function, even though the names of the variables are different,
just like $\int x\ dx$ and $\int y\ dy$ capture the same
anti-derivative). Because we wanted to minimise the complexity of the
task the neural networks needed to learn, we decided to use an
approach that abstracts away with explicit variable names, namely,
\textit{de Bruijn} indices, where integers are used to capture the
binding in terms of the structure of the term. For example,
$\lambda x.\lambda y.x$ becomes $\lambda.\lambda.1$ and
$\lambda x.\lambda y.y$ becomes $\lambda . \lambda . 0$.

Having implemented the STLC, we then implemented a type inferencer,
which given a lambda term infers a family of possible types. For
example, the lambda term $\verb!LAM NIL! \cong \lambda.*$ intuitively
takes an argument of unspecified type, and returns the term $*$ of
type unit (denoted ``1''), and so it has type $T \to 1$, where $T$ is
a type variable that can be instantiated with an arbitrary
type. Similarly, the term $\verb!LAM (VAR 0)! \cong \lambda.0$
intuivitely takes an argument of unspecified type, and returns the
same argument again, and so has type $T \to T$, where $T$ is as above.
Keeping in line with our desire to simplify the problem for the neural
networks, we instantiated all type holes with the unit type so as to
give us the simplest possible types to learn. We also have a type
checker which, given a term $t$ and a candidate type $T$, tells us
whether or not term $t$ can have type $T$.

We next implemented a randomized algorithm to generate our training
data. The training data consists of over one million pairs of terms
and types. We adopted a minimalistic syntax to capture terms and
types, and used prefix notation for types because we (intuitively, but
without any real scientific basis) believed it would make the tree
structure more apparent than infix notation (compare, e.g.,
$\verb!> T T'!$ to $\verb!T > T'!$ to denote the type $T \to T'$),
This is because the neural networks we consider process input
sequences sequentially, and we thought prefix notation might hint to
the network what kind of data to expect next.

We have been experimenting with various neural networks to learn this
inductive algorithm. For example, we have tried the ``seq2seq'' RNN
bundled with TensorFlow, which can allegedly successfully do
natural language translation. This RNN is composed of two parts. The
first is an ``encoder'' takes an input sequence, and encodes it (using
a representation it has learned on its own), and transforms it to
produce a ``context''. The second is the ``decoder'', which takes a
context as input, and produces a word. It then repeatedly produces a
new word given the context and the word it just produced until it
believes it is done. Because natural language and $\lambda$-terms and
types can all be parsed from sequences of tokens into syntax trees
(albeit of much different complexity), we believed this neural network
might be well-suited to the task of translating $\lambda$-terms (i.e.,
``sentences in the language of the $\lambda$-calculus'') into types
(i.e., ``sentences in the language of simple types'').

\section{Preliminary Results and Future Work}

The ``seq2seq'' approach has been unsuccessful. Though it usually
succeeded in generating well-formed types for given input
$\lambda$-terms, these types were invariably incorrect.

We will try training the above neural networks longer, and experiment
with different networks.
A few days ago, we learned of the NAMPI (Neural Abstract Machines \&
Program Induction) workshop at NIPS 2016, which is dedicated to
applying machine learning to inductive programming and program
synthesis. We hope to find some interesting neural networks (or
approaches to dealing with inductive structures) in its proceedings
that we can adapt to our problem.

Another direction will be pursuing is stack LSTMs~\cite{DyerBLMS15}.
Conventional LSTMs are limited in their applicability because
they model sequences in a left-to-right order, i.e. they read an input,
say $x_t$ and compute a new hidden state $h_t$ by applying a linear map
to the concatenation of $h_{t-1}$ and $x_t$. Lambda expressions
have a binary tree structure and can't be parsed in a left to right order.
For e.g., when parsing a function application, $e_1 \; e_2$, when
reading input $e_2$, the new hidden state should be computed from
the hidden state corresponding to $e_1$. Stack LSTMs
which maintain an additional stack pointer which decides which
LSTM cell will provide the the hidden state $h_{t-1}$ on which
the new hidden state $h_t$ will be computed. This seems
as a natural technique that should apply to our type inference
problem.

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}