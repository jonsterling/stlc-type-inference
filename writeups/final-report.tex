\documentclass[twocolumn]{article}
\usepackage{fullpage}
\usepackage{libertineRoman}
\usepackage{microtype}
\usepackage{libertineRoman}
\usepackage[utf8]{inputenc}
\usepackage{eulervm}
\usepackage{sectsty}
\usepackage[bw]{notation/modes}
\usepackage{notation/judgments}
\usepackage{notation/lists}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{cprotect}

\allsectionsfont{\sffamily}


\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{notation}[thm]{Notation}
\newtheorem{convention}[thm]{Convention}
\newtheorem{construction}[thm]{Construction}
\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\numberwithin{equation}{section}

\usepackage[
  backref,
  colorlinks,
  hyperfigures,
  hyperfootnotes,
  pagebackref,
  pdfdisplaydoctitle,
  pdfencoding=auto,
  psdextra,
  unicode,
  citecolor=NavyBlue
]{hyperref}

\newcommand\JdgDecl[2]{%
  \DeclBox{\IBox{#1}}%
  \ \ \textit{pronounced}\ \ %
  \DeclBox{\OBox{\text{#2}}}%
}

\newcommand\Abs[1]{\left\vert{#1}\right\vert}
\newcommand\Squares[1]{\left[#1\right]}

\newcommand\IsGEQ[2]{\IMode{#1}\geq\IMode{#2}}
\newcommand\IsLEQ[2]{\IMode{#1}\leq\IMode{#2}}

\newcommand\Nat{\mathbb{N}}
\newcommand\STLC{{\sffamily\bfseries{}STLC}}
\newcommand\FStar{\ensuremath{\text{F}^\star}}
\newcommand\RedPRL{\textbf{\textcolor{red}{Red}PRL}}

\newcommand\Nil{*}
\newcommand\Fn[2]{\mathtt{fn}\ {#1}\Rightarrow{#2}}
\newcommand\DBFn[1]{\mathtt{fn} (#1)}
\newcommand\TyUnit{\mathtt{unit}}
\newcommand\TyArr[2]{#1\to{#2}}
\newcommand\Var[1]{\overline{#1}}

\newcommand\OfTy[3]{\IMode{#1}\vdash\IMode{#2}:\IMode{#3}}

\newcommand\RuleVar{\mathsf{var}}
\newcommand\RuleNil{\mathsf{nil}}
\newcommand\RuleFn{\mathsf{fn}}
\newcommand\RuleAp{\mathsf{ap}}

\newcommand\Braces[1]{{\color{gray}\left\{#1\right\}}}
\newcommand\MkGoal[3]{\langle{#1}\vdash{#2}:{#3}\rangle}
\newcommand\MkEq[2]{\langle{#1}\doteq{#2}\rangle}
\newcommand\Decomp[3]{\IMode{#1}\longrightarrow\Braces{\OMode{#2}}\sslash\Braces{\OMode{#3}}}
\newcommand\ADecomp[3]{\IMode{#1}&\longrightarrow\Braces{\OMode{#2}}\sslash\Braces{\OMode{#3}}}
\newcommand\StEval[3]{\Braces{\IMode{#1}}\sslash\Braces{\IMode{#2}}\Longrightarrow\Braces{\OMode{#3}}}
\newcommand\LocalUnify[3]{\IMode{#1}\longrightarrow\Braces{\OMode{#2}}\triangleright\Braces{\OMode{#3}}}
\newcommand\ALocalUnify[3]{\IMode{#1}&\longrightarrow\Braces{\OMode{#2}}\triangleright\Braces{\OMode{#3}}}
\newcommand\NoOccurs[2]{\IMode{#1}\mathrel{\text{does not occur in}}\IMode{#2}}
\newcommand\Unify[3]{\Braces{\IMode{#1}}\triangleright\Braces{\IMode{#2}}\Longrightarrow\Braces{\OMode{#3}}}
\newcommand\InferTy[3]{\IMode{#1}\vdash\IMode{#2}\Rightarrow\OMode{#3}}
\newcommand\Subst[2]{\widehat{#1} (#2)}


\newcommand{\RNN}[2]{\text{RNN}(#1, #2)}
\newcommand{\RNNf}[2]{\overrightarrow{\text{RNN}}(#1, #2)}
\newcommand{\RNNb}[2]{\overleftarrow{\text{RNN}}(#1, #2)}
\newcommand{\st}[1]{\mathbf{s}_{#1}}
\newcommand{\Ht}[1]{\mathbf{h}_{#1}}
\newcommand{\Htf}[1]{\overrightarrow{\mathbf{h}}_{#1}}
\newcommand{\Htb}[1]{\overleftarrow{\mathbf{h}}_{#1}}
\newcommand{\Htd}[1]{\mathbf{h}^{\tau}_{#1}}
\newcommand{\Al}[1]{\alpha_{#1}}


\title{Learning Simple Types}
\author{Costin B\u{a}descu\and Ankush Das\and Ryan Kavanagh\and Jon Sterling}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Type structure is the fundamental organizing principle of code, both
in theory and in practice. Types express the range of significance of
a piece of code: for instance, the function $\IMode{\Fn{x}{x+1}}$ is
meaningful only when applied to an expression which supports an
addition operation.

Depending on the purpose of a programming language, different forms of
type structure may be imposed; suitable implementation strategies for
type systems vary wildly depending on their characteristics.

\begin{enumerate}
\item Some type systems (including \emph{simple types} and
  \emph{prenex-polymorphic types}) support \textbf{type inference}: an
  algorithm which either assigns a type to a piece of code, or
  exhibits that code as ill-typed.
\item More sophisticated type systems (including full
  \emph{polymorphic types} and \emph{intensional dependent types})
  support \textbf{type checking}: an algorithm which decides whether a
  piece of code exhibits a \emph{given} type.
\item Even more sophisticated type systems (such as \emph{extensional
    dependent types}) support only \textbf{type proving}: given a
  piece of code, a type, and a candidate derivation (proof) that this
  code exhibits this type, we can decide whether this derivation is
  correct.\footnote{In this case, type \emph{checking} may not even be
    semidecidable, because the typing relation is semantic rather than
    being defined by a recursively enumerable set of rules.}
\end{enumerate}

As can be seen above, available implementation strategies dwindle as
type systems become more sophisticated, eventually converging at a
state of affairs where a practitioner must supply a proof of
well-typedness. However, this does not rule out the use of
\emph{heuristics} in sophisticated type systems which achieve type
inference or type checking in some cases, but fail in others.

In some implementations of dependent type theory such as
Nuprl~\cite{constable:1986} and \RedPRL{}~\cite{redprl:2016}, these
heuristics take the form of \emph{proof
  tactics}~\cite{gordon-milner-wadsworth:1979,paulson:1987,gordon:2000}. It
is also possible to consider heuristics based on external SMT solvers
as in the \FStar{}~language~\cite{ahman-et-al:2017}, which uses the Z3
solver to resolve obligations.

Finally, machine learning techniques have also been applied to
interactive proof assistants such as
Coq~\cite{komendantskaya-heras-grov:2012} to prove \emph{proof hints}
during interactive proof development: for instance, Komendantskaya and
Heras' system can suggest auxiliary lemmas or tactics to be applied,
based on context.

\paragraph{Thesis}
We believe that at the current stage of scientific development, where
fully-automated inference is no longer possible, the principal
contradiction of type systems praxis lies in the development of
effective heuristics for type inference and type checking. To this
end, we have applied techniques from \emph{deep learning} to address a
restricted subproblem, namely type inference for the simply typed
lambda calculus (\STLC{}) with a single base type.

\section{Simply Typed Lambda Calculus}
The simply typed lambda calculus (\STLC{}) is the simplest possible
programming language, and provides a suitable environment in which to
test our ideas before applying them to more difficult problems. In
Figure~\ref{fig:stlc-grammar-and-declarative-typing}, we define the
syntax and \emph{declarative} typing rules for the \STLC{}.

\paragraph{Declarative vs algorithmic typing}
Declarative typing judgments give a mathematical definition of the
typing relation for programs; such a definition is not usually
suitable for implementation. In the case of \STLC{}, we will define an
algorithmic \emph{type inference} relation which is adequate (sound
and complete) with respect to the declarative typing relation.

\begin{figure*}
  \begin{align*}
    \IMode{\tau} &::= \FormatList{\OMode}{\mid}{\TyUnit,\TyArr{\IMode{\tau}}{\IMode{\tau}}}
    \tag{Types}
    \\
    \IMode{e} &::= \FormatList{\OMode}{\mid}{x, \Nil, \Fn{\IMode{x}}{\IMode{e_x}}, \IMode{e}(\IMode{e})}
    \tag{Programs}
    \\
    \IMode{\Gamma} &::= \FormatList{\OMode}{\mid}{\cdot,{\IMode{\Gamma},\IMode{x}:\IMode{\tau}}}
    \tag{Typing contexts}
  \end{align*}
  \begin{gather*}
    \JdgDecl{
      \OfTy{\Gamma}{e}{\tau}
    }{
      $\IMode{e}$ is of type $\IMode{\tau}$ in context $\IMode{\Gamma}$
    }
    \\[6pt]
    \infer[\RuleVar]{
      \OfTy{\Gamma,x:\tau,\Gamma'}{x}{\tau}
    }{
    }
    \qquad
    \infer[\RuleNil]{
      \OfTy{\Gamma}{\Nil}{\TyUnit}
    }{
    }
    \\[6pt]
    \infer[\RuleFn]{
      \OfTy{\Gamma}{\Fn{x}{e_x}}{\TyArr{\tau_1}{\tau_2}}
    }{
      \OfTy{\Gamma,x:\tau_1}{e_x}{\tau_2}
    }
    \qquad
    \infer[\RuleAp]{
      \OfTy{\Gamma}{e_1(e_2)}{\tau_2}
    }{
      \OfTy{\Gamma}{e_1}{\TyArr{\tau_1}{\tau_2}}
      &
      \OfTy{\Gamma}{e_2}{\tau_1}
    }
  \end{gather*}

  \caption{Grammar and declarative typing rules for the \STLC{}. In
    this definition, we use names for bound variables and indicate
    binding scope using
    subscripts.}\label{fig:stlc-grammar-and-declarative-typing}
\end{figure*}

\subsection{Algorithmic type inference}

In Figure~\ref{fig:stlc-algorithmic-grammar}, we give a version of the
grammar of \STLC{} augmented with \emph{type metavariables}, which
play a crucial role in the type inference process.
Our grammar also includes syntax
for the states of the type inference machine, including typing goals,
typing goal stacks, unification equations, unification constraint
sets, and type substitutions.

\paragraph{De Bruijn Indices}
Additionally, in this treatment we use De Bruijn indices rather than
names to account for binding; De Bruijn indices reflect the
address/location of a variable's binding in a syntax tree.
%
Among other things, De Bruijn presentations of calculi with binding
structure are easier to work with computationally, because they
eliminate the need to consider equivalence classes of terms under
permutations of bound variable names.

In a De Bruijn presentation, operators with binding structure do not
bind a \emph{name} (as in $\Fn{x}{\ldots}$); instead, the arguments to
a binding operator are specified by a numeric offset (called a
``valence'') which says \emph{how many} variables are to bound. In the
case of the functional abstraction operator, the De Bruijn
presentation is $\DBFn{\ldots}$, and the valence of its argument is
$1$.

Then, a variable is written as a numeral which specifies the number of
binders between it and its binder. For example, the term
$\Fn{x}{\Fn{y}{x(y)}}$ is written
$\DBFn{\DBFn{\overline{1}(\overline{0})}}$ in De Bruijn notation.

\paragraph{Type inference algorithm}



\begin{figure*}
  \begin{gather*}
    \begin{aligned}
      \IMode{\tau} &::= \FormatList{\OMode}{\mid}{\alpha, \TyUnit,\TyArr{\IMode{\tau}}{\IMode{\tau}}}
      &\text{(Types)}
      \\
      \IMode{e} &::= \FormatList{\OMode}{\mid}{\Var{n}, \Nil, \DBFn{\IMode{e}}, \IMode{e}(\IMode{e})}
      &\text{(Programs)}
      \\
      \IMode{\Gamma} &::= \FormatList{\OMode}{\mid}{\cdot,{\IMode{\Gamma},\IMode{\tau}}}
      &\text{(Typing contexts)}
      \\
      \IMode{\mathcal{G}} &::= \OMode{\MkGoal{\IMode{\Gamma}}{\IMode{e}}{\IMode{\tau}}}
      &\text{(Typing goals)}
    \end{aligned}
    \qquad\qquad
    \begin{aligned}
      \IMode{\mathcal{S}} &::= \FormatList{\OMode}{\mid}{\cdot, {\IMode{\mathcal{G}},\IMode{\mathcal{S}}}}
      &\text{(Typing stacks)}
      \\
      \IMode{\mathcal{E}} &::= \OMode{\MkEq{\IMode{e}}{\IMode{e}}}
      &\text{(Unification equations)}
      \\
      \IMode{\mathcal{C}} &::= \FormatList{\OMode}{\mid}{\cdot,{\IMode{\mathcal{E}},\IMode{\mathcal{C}}}}
      &\text{(Unification constraints)}
      \\
      \IMode{\gamma} &::= \FormatList{\OMode}{\mid}{\cdot, {\IMode{\alpha}\hookrightarrow\IMode{\tau},\IMode{\gamma}}}
      &\text{(Substitutions)}
    \end{aligned}    
  \end{gather*}


  Write $\IMode{\Subst{\gamma}{-}}$ for the action of the substitution
  $\IMode{\gamma}$, which proceeds by replacing $\IMode{\alpha}$ with
  $\OMode{\tau}$ when $\IMode{\alpha\hookrightarrow\tau}$ is in
  $\IMode{\gamma}$. This notation applies at all syntactic categories.

  \caption{Grammar of algorithmic \STLC{} used in algorithmic type
    inference.  For this presentation, we use \emph{de Bruijn indices}
    $\IMode{\Var{n}}$ with $\Member{n}{\Nat}$ rather than names for
    program variables. Observe that we have added \emph{metavariables}
    $\IMode{\alpha}$ to the grammar of types, which will serve as
    placeholders to be resolved during unification.
  }\label{fig:stlc-algorithmic-grammar}
\end{figure*}

Next, in Figure~\ref{fig:algorithmic-type-inference} we give the full
type inference algorithm for \STLC{} in terms of constraint generation
and first-order unification. We have used this algorithm in our
implementation in order to generate training data for our neural
network.

\begin{figure*}
  \begin{gather*}
    \JdgDecl{
      \Decomp{\mathcal{G}}{\mathcal{S}}{\mathcal{C}}
    }{
      $\IMode{\mathcal{G}}$ decomposes into $\OMode{\mathcal{S}}$ with constraints $\OMode{\mathcal{C}}$
    }
    \\[6pt]
    \begin{aligned}
      \ADecomp{
        \MkGoal{\Gamma}{\Var{n}}{\tau}
      }{
        \cdot
      }{
        \MkEq{\tau}{\Gamma_n}
      }
      \\
      \ADecomp{
        \MkGoal{\Gamma}{\Nil}{\tau}
      }{
        \cdot
      }{
        \MkEq{\tau}{\TyUnit}
      }
      \\
      \ADecomp{
        \MkGoal{\Gamma}{\DBFn{e}}{\tau}
      }{
        \MkGoal{\Gamma,\alpha}{e}{\beta}
      }{
        \MkEq{\tau}{\TyArr{\alpha}{\beta}}
      }
      \\
      \ADecomp{
        \MkGoal{\Gamma}{e_1(e_2)}{\tau}
      }{
        \MkGoal{\Gamma}{e_1}{\TyArr{\alpha}{\tau}},
        \MkGoal{\Gamma}{e_2}{\alpha}
      }{
        \cdot
      }
    \end{aligned}
  \end{gather*}

  \begin{gather*}
    \JdgDecl{
      \StEval{\mathcal{S}}{\mathcal{C}}{\mathcal{C}'}
    }{
      $\IMode{\mathcal{S}}$ with constraints $\IMode{\mathcal{C}}$ generates constraints $\OMode{\mathcal{C}'}$
    }
    \\[6pt]
    \infer{
      \StEval{\cdot}{\mathcal{C}}{\mathcal{C}}
    }{
    }
    \qquad
    \infer{
      \StEval{\mathcal{G},\mathcal{S}}{\mathcal{C}}{\mathcal{C}'}
    }{
      \Decomp{\mathcal{G}}{\mathcal{S}_{\mathcal{G}}}{\mathcal{C}_{\mathcal{G}}}
      &
      \StEval{\mathcal{S}\cup\mathcal{S}_{\mathcal{G}}}{\mathcal{C}\cup\mathcal{C}_{\mathcal{G}}}{\mathcal{C}'}
    }
  \end{gather*}

  \begin{gather*}
    \JdgDecl{
      \LocalUnify{\mathcal{E}}{\mathcal{C}}{\gamma}
    }{
      $\IMode{\mathcal{E}}$ generates additional constraints $\OMode{\mathcal{C}}$ and substitution $\OMode{\gamma}$
    }
    \\[6pt]
    \infer{
      \LocalUnify{
        \MkEq{\alpha}{\tau}
      }{
        \cdot
      }{
        \alpha\hookrightarrow\tau
      }
    }{
      \NoOccurs{\alpha}{\tau}
    }
    \qquad
    \infer{
      \LocalUnify{
        \MkEq{\tau}{\alpha}
      }{
        \cdot
      }{
        \alpha\hookrightarrow\tau
      }
    }{
      \NoOccurs{\alpha}{\tau}
    }
    \\[6pt]
    \begin{aligned}
      \ALocalUnify{\MkEq{\TyUnit}{\TyUnit}}{\cdot}{\cdot}
      \\
      \ALocalUnify{\MkEq{\TyArr{\tau_1}{\tau_2}}{\TyArr{\tau_1'}{\tau_2'}}}{
        \MkEq{\tau_1}{\tau_1'},
        \MkEq{\tau_2}{\tau_2'}
      }{\cdot}
    \end{aligned}
  \end{gather*}

  \begin{gather*}
    \JdgDecl{
      \Unify{\gamma}{\mathcal{C}}{\gamma'}
    }{
      constraints $\IMode{\mathcal{C}}$ take substitution $\IMode{\gamma}$ to $\OMode{\gamma'}$
    }
    \\[6pt]
    \infer{
      \Unify{\gamma}{\cdot}{\gamma}
    }{
    }
    \qquad
    \infer{
      \Unify{\gamma}{\mathcal{E},\mathcal{C}}{\gamma'}
    }{
      \LocalUnify{\mathcal{E}}{\mathcal{C}_{\mathcal{E}}}{\gamma_{\mathcal{E}}}
      &
      \Unify{\gamma\cup\gamma_{\mathcal{E}}}{\Subst{\gamma_{\mathcal{E}}}{\mathcal{C}}\cup\mathcal{C}_{\mathcal{E}}}{\gamma'}
    }
  \end{gather*}

  \begin{gather*}
    \JdgDecl{
      \InferTy{\Gamma}{e}{\tau}
    }{
      the term $\IMode{e}$ infers type $\OMode{\tau}$ given context $\IMode{\Gamma}$
    }
    \\[6pt]
    \infer{
      \InferTy{\Gamma}{e}{\Subst{\gamma}{\alpha}}
    }{
      \StEval{
        \MkGoal{\Gamma}{e}{\alpha}
      }{\cdot}{\mathcal{C}}
      &
      \Unify{\cdot}{\mathcal{C}}{\gamma}
    }
  \end{gather*}

  \caption{Definition of algorithmic type inference for \STLC{}. We
    have factored the type inference algorithm into several judgments:
    local decomposition of typing problems, evaluation of typing
    problem stacks into constraint sets, local decomposition of
    equational constraints, and evaluation of constraint sets into
    substitutions. We have implemented this algorithm in the
    \textbf{Standard~ML} programming
    language.}\label{fig:algorithmic-type-inference}
\end{figure*}


\section{Training Data Generation}

We implemented a na√Øve randomized algorithm to generate over one
million unique pairs of terms and their inferred types. This algorithm
randomly decided which term former to use, that is, whether to
generate an application, an abstraction, a variable, or a unit
term. In the cases where it chose to generate an application or an
abstraction, it then recursed to generate the required subterms. A
subtle point in the generation of subterms for abstractions was that
the left term must have a function type for the abstraction to be
well-typed. Generating a sufficient variety of terms required
carefully tuning the probabilities associated with each type former.

To reduce complexity, we adopted a minimalistic concrete syntax for
terms and types and we instantiated all type variables with the unit
type so as to give us the simplest possible types to learn. For example,
when the type inference algorithm infers the type
$\OMode{\TyArr\alpha\alpha}$ for the term $\IMode{\Fn{x}{x}}$ for some
type variable $\OMode{\alpha}$, we assigned the term the type
$\OMode{\TyArr\TyUnit\TyUnit}$.

\section{Neural Network Models}
We introduce the main neural network model that we have used for type inference
of terms from the Simply Typed Lambda Calculus (\STLC{}). We introduce
some preliminaries that we need to explain encoder-decoder models that
have essentially been used for type inference in our setting.

\subsection{Recurrent Neural Networks (RNNs)}
RNNs~\cite{Elman90RNN} are a special class of neural networks which
are used to model long-distance dependencies. If we treat the source
expression $e$ as a sequence of embeddings ${(\st{t})}_{t \geq 1}$,
with embedding $\st{t}$ at time step $t$, then an RNN simply
adds a connection to the previous hidden state when computing the
current hidden state.
\[
  \Define{
    \Ht{t}
  }{
    \begin{cases}
      \tanh(W_{sh}\st{t} + W_{hh}\Ht{t-1} + \mathbf{b}_h) & \IsGEQ{t}{q}\\
      \mathbf{0} & \text{otherwise}
    \end{cases}
  }
\]
The above equation is often abbreviated as
$\Define{\Ht{t}}{\RNN{\st{t}}{\Ht{t-1}}}$.  The matrices
$W_{sh}, W_{hh}$ and vectors $b_h$ are parameters that are learned
during training. RNNs are able to model long distance dependencies
because they can pass information between time steps.  Typically, RNNs
suffer from the problem of vanishing and exploding gradients, hence we
use a robust variant of RNN called Gated Recurrent Unit
(GRU)~\cite{GRU-NIPS14}.

\subsection{Encoder-Decoder Models}
We give a brief overview of encoder-decoder
models~\cite{SutskeverSeq2Seq} with more details of the specific
architecture used to solve our type inference problem.  Given a source
expression $e$ and target type $\tau$, these models are used to model
the probability $\mathsf{P}(\tau \mid e)$. The basic idea of the model
is the following: a first neural network (the encoder) takes the
source expression $e$ and encodes it as a vector of real-valued
numbers, while a second neural network (the decoder) takes this vector
as input and decodes it to generate a target type $\tau$.

We first describe the encoding phase. We use a bidirectional RNN
encoder~\cite{BiRNN} with a GRU cell. In this method, we use two
different encoders: one traveling forward and another traveling
backward over the input sequence of embeddings
${(\st{t})}_{t \geq 1}$.

\begin{align*}
  \ADefine{
    \Htf{t}
  }{
    \begin{cases}
      \RNNf{\st{t}}{\Htf{t-1}}& \IsGEQ{t}{1} \\
      \mathbf{0} & \text{otherwise}
    \end{cases}
  }
  \\
  \ADefine{
    \Htb{t}
  }{
    \begin{cases}
      \RNNb{\st{t}}{\Htb{t+1}}& \IsLEQ{t}{\Abs{e}}\\
      \mathbf{0} & \text{otherwise}
    \end{cases}
  }
\end{align*}

The initial vector $\Htd{0}$ for the decoder RNN is obtained by simply
concatenating the vectors $\Htf{\Abs{e}}$ and $\Htb{1}$. Hence, the
bidirectional representation of the source expression is the
concatenation of the forward and backward units.
\[
  \Define{
    \Ht{t}
  }{
    \Squares{\Htf{t} ; \Htb{t}}
  }
\]
We can further concatenate these vectors into a matrix where every
column corresponds to one character in the source expression.
\[
  \Define{H}{
    \mathsf{concat\_col}\left(\Ht{1}, \Ht{2}, \ldots, \Ht{\Abs{e}}\right)
  }
\]
That describes the encoder model. We use an attention decoder
with a GRU cell to generate
the type from this bidirectional representation. This decoder calculates
a vector $\Al{t}$ that can be used to combine together the columns of $H$
into a vector $\Define{\mathbf{c}_t}{H \Al{t}}$. The attention vector basically denotes
the focusing on a particular source word at a particular time step. The larger
the $\Al{t}$, the more impact a character will have in predicting the next
character in the type. These $\Al{t}$s are computed while tracking the state
in the decoder RNN. Suppose the output type is a sequence of embeddings
${(\tau_t)}_{t \geq 1}$ with the embedding $\tau_t$ at time step $t$.
The hidden state in the decoder $\Htd{t}$ is computed
using the following.
\[
\Define{\Htd{t}}{\mathsf{enc}([\tau_{t-1} ; \mathbf{c}_{t-1}], \Htd{t-1})}
\]
Based on this hidden state $\Htd{t}$, we calculate an attention score $\Al{t}$
\[
\Define{\Al{t, j}}{\mathsf{attn\_score}(\Ht{j}, \Htd{t})}
\]
where attn\_score is an arbitrary function that takes two vectors as input,
and produces the focusing as output, quite often just computing the dot
product of the two vectors.

That concludes the description of the decoder used in our model. With this
bidirectional RNN encoder and attention decoder, we train the model for
type inference.

\section{Implementation and Results}
We have used the open source implementation \texttt{tf-seq2seq}~\cite{Britz2017}
from Google. \texttt{tf-seq2seq} is a general purpose encoder-decoder framework
for Tensorflow. This framework can be used for machine translation, text
summarization, image captioning, etc. We have used this framework
for type inference of expressions in \STLC{}, i.e.\ given expression $e$ in
\STLC{} as a sequence of characters, we produce a type $\tau$ as a sequence
of characters. 

We used the same complexity of the neural network for
each task. As discussed earlier, we used a bidirectional RNN encoder
with an attention decoder. We set the number of units in the attention
layer to 128. The embedding dimension is also 128. The number of
layers both in the encoder and decoder is 1. The input dropout
probability is 0.8 while the output dropout probability is 1 in both
the encoder and decoder. We trained our model for 10,000 steps
and used a batch size of 32.

Since our goal was to show \textit{possibility}, we started out
with the simplest task possible, and gradually increased the difficulty of the
tasks performed. We list each task below with the accuracy of our model.

\paragraph{Expressions of lambda depth 1}
We first restricted expressions to be of lambda depth 1. This means that
only the innermost lambda abstracted variable can be used in an expression.
For instance, in the expression $\lambda x. \lambda y. e$, $e$ can only
refer to $y$, the innermost lambda abstracted variable. In the abstract
syntax tree, this creates a lambda depth of at most 1. In terms of De Bruijn
index representation, this means that the only numeral that can occur in
the string representation of the expression is 0. This restricts the vocabulary
of the source expressions to only the characters $(, ), \backslash, 0$.
We also restrict the expressions to have only two types $1$ and $\TyArr{1}{1}$.
With this fairly restricted model, we trained our encoder-decoder model for
type inference. We generated 11,882 unique expression-type pairs, and split the
data into 3 sets, 80\% training set, 10\% validation set and 10\% test set.
Finally, while producing the output, we used 1-best search, i.e.\ we generated
the output sequence which had the highest probability given the input
sequence. With this simple problem, our accuracy was 100\% on the test
set. With this remarkable accuracy, we increased the difficulty of the problem.

\paragraph{Classification into 3 types}
We then performed classification into 3 types, $1$, $\TyArr{1}{1}$ and
$\TyArr{1}{(\TyArr{1}{1})}$. We used the same splitting proportion with
the same parameters that we used in the previous task. We generated
18,337 expression-type pairs to do type inference. Again, we restricted
the expression to be of lambda depth 1. With this setting, the accuracy
on the test set is 98.78\%.

\paragraph{Arbitrary expressions}
With this success, we moved to solving a fairly difficult problem where
we did not restrict the lambda depth of the expression. We restricted the
number of types to only two, i.e.\ $1$ and $\TyArr{1}{1}$. The vocabulary
of the source expression was $(, ), \backslash, 0, 1, 2, \ldots, 19$, i.e.\
the maximum depth of 19. We generated 137,936 expression-type pairs
and obtained an accuracy of 98.49\% on the test set.

%We used Google's \texttt{seq2seq} library, a general-purpose
%encoder-decoder framework for Tensorflow, as the basis for our
%implementation. We approached the problem of learning type inference by
%solving several classification problems of increasing difficulty.
%
%We first considered the binary classification problem of distinguishing
%whether an expression of depth $1$ is of type $1$ or $1 \to 1$. We used
%$11,882$ input-output pairs, a training set of size $80\%$, a validation
%set of size $10\%$, and a test set of size $10\%$. After $10,000$
%training steps, the model was able to correctly classify all of the
%samples in our test set.
%
%We then tried to classify terms of depth $1$ into $3$ types $1$, $1 \to
%1$, and $1 \to (1 \to 1)$. Using $18,337$ input-output pairs, our model
%predicts the correct type with $98.78\%$ accuracy.
%
%We also tried to classify expressions of arbitrary depth into $2$ types
%and achieved an accuracy of $98.49\%$ using $137,936$ input-output
%pairs.

\paragraph{Arbitrary expressions and types}
Finally, we considered the most general problem of inferring the types of
arbitrary expressions using our encoder-decoder model. We treated this as a
translation problem (instead of classifcation)
and provided the framework with $100,000$
training samples of expression-type pairs.
We used $10,000$ test samples. After training the neural network
for $10,000$ steps, the model correctly predicts the type of $77.1\%$ of
the expressions in the test set.

These results show that encoder-decoder frameworks can be successfully
used in a restricted setting where the number of types are limited,
and that they work remarkably well even when the number of possible
types is unrestricted. Since the network sizes we have used in this
experiment are fairly small (due to our limited computation power), we
believe more complicated networks can provide higher accuracies.

\section{Conclusion}
We learned that encoder-decoder models are a viable framework
for type inference. Since type inference, at its core, is a generation
problem, rather than a classification problem, these models are a
natural neural network choice. We have seen that, in the restricted
setting of classification, these models performed extremely well
with an accuracy above 98\%. During generation, a fairly small
network model was able to generate the correct type with a reasonable
accuracy of 77\%. Thus, deep learning can serve as a possible
heuristic in producing types for arbitrary expressions.

\bibliographystyle{abbrv}
\bibliography{refs,references/refs}

\end{document}
